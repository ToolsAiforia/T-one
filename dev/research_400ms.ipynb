{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc464339",
   "metadata": {},
   "source": [
    "# Проверяем можно ли пересобрать модельку в onnx с 400мс буфером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcb80855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tone import StreamingCTCPipeline, read_stream_example_audio\n",
    "\n",
    "\n",
    "pipeline = StreamingCTCPipeline.from_hugging_face()\n",
    "\n",
    "# state = None  # Current state of the ASR pipeline (None - initial)\n",
    "# for audio_chunk in read_stream_example_audio():  # Use any source of audio chunks\n",
    "#     new_phrases, state = pipeline.forward(audio_chunk, state)\n",
    "#     # print(new_phrases)\n",
    "\n",
    "# # Finalize the pipeline and get the remaining phrases\n",
    "# new_phrases, _ = pipeline.finalize(state)\n",
    "# print(new_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eb4d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Хорошо, давай попробуем теперь скачать модельку локально и по-трейсить\n",
    "from pathlib import Path\n",
    "resources_path = Path(\"./model\")\n",
    "resources_path.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "# StreamingCTCPipeline.download_from_hugging_face(dir_path=\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ca5865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -m tone.scripts.export --chunk-duration-ms 300 --output_path ./model_300ms.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e4ac3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tone.scripts.export as tone_onnx_export\n",
    "\n",
    "path_to_pretrained = \"t-tech/T-one\"\n",
    "chunk_duration_ms = 300\n",
    "\n",
    "# model = tone_onnx_export.ModelToExport(path_to_pretrained, chunk_duration_ms)\n",
    "# model_bytes = tone_onnx_export._export_onnx(model)\n",
    "\n",
    "# output_path = Path(\"./model_300ms.onnx\")\n",
    "# output_path.write_bytes(model_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b39fc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal.shape=torch.Size([5, 3200, 1])\n",
      "signal.shape=torch.Size([5, 3200, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "144198453"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_duration_ms = 400\n",
    "\n",
    "model = tone_onnx_export.ModelToExport(path_to_pretrained, chunk_duration_ms)\n",
    "model_bytes = tone_onnx_export._export_onnx(model)\n",
    "\n",
    "output_path = Path(f\"./model_{chunk_duration_ms}ms.onnx\")\n",
    "output_path.write_bytes(model_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b29b00",
   "metadata": {},
   "source": [
    "# Проверяем метрики в стриминге\n",
    "\n",
    "как там wer считать ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b0ef5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-12-22 13:01:08.492047315 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 38 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-12-22 13:01:08.500086461 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-12-22 13:01:08.500094601 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from tone import read_stream_audio, StreamingCTCPipeline, StreamingCTCModel\n",
    "from nemo.collections.asr.metrics.wer import word_error_rate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# StreamingCTCPipeline.download_from_hugging_face(\"./model\")\n",
    "# pipeline = StreamingCTCPipeline.from_local(\"./model\", providers=['CUDAExecutionProvider'])\n",
    "\n",
    "StreamingCTCModel.AUDIO_CHUNK_SAMPLES = int(0.4 * StreamingCTCModel.SAMPLE_RATE)\n",
    "StreamingCTCPipeline.CHUNK_SIZE = StreamingCTCModel.AUDIO_CHUNK_SAMPLES\n",
    "pipeline = StreamingCTCPipeline.from_local(\"./model_400ms_fixed/streaming_acoustic/1\", providers=['CUDAExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17943a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/264 [00:02<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 219729)\n",
      "3\n",
      "(1, 219729)\n",
      "3\n",
      "(1, 219729)\n",
      "16\n",
      "(1, 219729)\n",
      "29\n",
      "(1, 219729)\n",
      "42\n",
      "(1, 219729)\n",
      "3\n",
      "(1, 219729)\n",
      "16\n",
      "(1, 219729)\n",
      "29\n",
      "(1, 219729)\n",
      "42\n",
      "(1, 219729)\n",
      "55\n",
      "(1, 219729)\n",
      "68\n",
      "(1, 219729)\n",
      "81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_hypos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     37\u001b[39m     hypos.append(clean_text(pred_text))\n\u001b[32m     39\u001b[39m     gts.append(clean_text(_meta[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mall_hypos\u001b[49m += hypos\n\u001b[32m     42\u001b[39m all_gts += gts\n\u001b[32m     43\u001b[39m word_error_rate(hypos, gts)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_hypos' is not defined"
     ]
    }
   ],
   "source": [
    "SUB = str.maketrans('ё', 'е')\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Lowercase a string and substitute.\"\"\"\n",
    "    return s.lower().translate(SUB)\n",
    "base_data_path = Path(\"/home/mle/aiphoria-asr-training/data/rus_finetune\")\n",
    "\n",
    "dataset_name = \"test_collection\"\n",
    "# load collections manifest\n",
    "with open(base_data_path / dataset_name / \"tarred_audio_manifest.json\") as fin:\n",
    "    jdata = []\n",
    "    for line in fin:\n",
    "        jline = json.loads(line.strip())\n",
    "        jdata.append(jline)\n",
    "len(jdata)\n",
    "\n",
    "hypos = []\n",
    "gts = []\n",
    "for _meta in tqdm(jdata):\n",
    "    audio_path = base_data_path / dataset_name / Path(_meta[\"audio_filepath\"]).name\n",
    "    \n",
    "    state = None\n",
    "    chunk_phrases = []\n",
    "    for audio_chunk in read_stream_audio(path_to_file=audio_path, chunk_size=pipeline.CHUNK_SIZE):\n",
    "        new_phrases, state = pipeline.forward(audio_chunk, state)\n",
    "        # print(state)\n",
    "        print(state[0].shape)\n",
    "        print(state[1].past_logprobs.shape[0])\n",
    "        if new_phrases:\n",
    "            chunk_phrases += new_phrases\n",
    "\n",
    "    break\n",
    "    \n",
    "    # Finalize the pipeline and get the remaining phrases\n",
    "    new_phrases, _ = pipeline.finalize(state)\n",
    "    output = chunk_phrases + new_phrases\n",
    "    pred_text = \" \".join([phrase.text for phrase in output])\n",
    "    hypos.append(clean_text(pred_text))\n",
    "    \n",
    "    gts.append(clean_text(_meta[\"text\"]))\n",
    "\n",
    "all_hypos += hypos\n",
    "all_gts += gts\n",
    "word_error_rate(hypos, gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef5eb1c",
   "metadata": {},
   "source": [
    "# Теперь считаем нормальные тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ae2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hypos = []\n",
    "all_gts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ebc637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:33<00:00,  7.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15128504672897197"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUB = str.maketrans('ё', 'е')\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Lowercase a string and substitute.\"\"\"\n",
    "    return s.lower().translate(SUB)\n",
    "base_data_path = Path(\"/home/mle/aiphoria-asr-training/data/rus_finetune\")\n",
    "\n",
    "dataset_name = \"test_collection\"\n",
    "# load collections manifest\n",
    "with open(base_data_path / dataset_name / \"tarred_audio_manifest.json\") as fin:\n",
    "    jdata = []\n",
    "    for line in fin:\n",
    "        jline = json.loads(line.strip())\n",
    "        jdata.append(jline)\n",
    "len(jdata)\n",
    "\n",
    "hypos = []\n",
    "gts = []\n",
    "for _meta in tqdm(jdata):\n",
    "    audio_path = base_data_path / dataset_name / Path(_meta[\"audio_filepath\"]).name\n",
    "    \n",
    "    state = None\n",
    "    chunk_phrases = []\n",
    "    for audio_chunk in read_stream_audio(path_to_file=audio_path, chunk_size=pipeline.CHUNK_SIZE):\n",
    "        new_phrases, state = pipeline.forward(audio_chunk, state)\n",
    "        if new_phrases:\n",
    "            chunk_phrases += new_phrases\n",
    "    # Finalize the pipeline and get the remaining phrases\n",
    "    new_phrases, _ = pipeline.finalize(state)\n",
    "    output = chunk_phrases + new_phrases\n",
    "    pred_text = \" \".join([phrase.text for phrase in output])\n",
    "    hypos.append(clean_text(pred_text))\n",
    "    \n",
    "    gts.append(clean_text(_meta[\"text\"]))\n",
    "\n",
    "all_hypos += hypos\n",
    "all_gts += gts\n",
    "word_error_rate(hypos, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "642581b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311/311 [00:55<00:00,  5.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12844488188976377"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"test_rec_support\"\n",
    "# load collections manifest\n",
    "with open(base_data_path / dataset_name / \"tarred_audio_manifest.json\") as fin:\n",
    "    jdata = []\n",
    "    for line in fin:\n",
    "        jline = json.loads(line.strip())\n",
    "        jdata.append(jline)\n",
    "len(jdata)\n",
    "\n",
    "hypos = []\n",
    "gts = []\n",
    "for _meta in tqdm(jdata):\n",
    "    audio_path = base_data_path / dataset_name / Path(_meta[\"audio_filepath\"]).name\n",
    "    \n",
    "    state = None\n",
    "    chunk_phrases = []\n",
    "    for audio_chunk in read_stream_audio(path_to_file=audio_path, chunk_size=pipeline.CHUNK_SIZE):\n",
    "        new_phrases, state = pipeline.forward(audio_chunk, state)\n",
    "        if new_phrases:\n",
    "            chunk_phrases += new_phrases\n",
    "    # Finalize the pipeline and get the remaining phrases\n",
    "    new_phrases, _ = pipeline.finalize(state)\n",
    "    output = chunk_phrases + new_phrases\n",
    "    pred_text = \" \".join([phrase.text for phrase in output])\n",
    "    hypos.append(clean_text(pred_text))\n",
    "    \n",
    "    gts.append(clean_text(_meta[\"text\"]))\n",
    "\n",
    "all_hypos += hypos\n",
    "all_gts += gts\n",
    "word_error_rate(hypos, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e419100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [00:37<00:00,  8.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19408054342552158"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"test_support\"\n",
    "# load collections manifest\n",
    "with open(base_data_path / dataset_name / \"tarred_audio_manifest.json\") as fin:\n",
    "    jdata = []\n",
    "    for line in fin:\n",
    "        jline = json.loads(line.strip())\n",
    "        jdata.append(jline)\n",
    "len(jdata)\n",
    "\n",
    "hypos = []\n",
    "gts = []\n",
    "for _meta in tqdm(jdata):\n",
    "    audio_path = base_data_path / dataset_name / Path(_meta[\"audio_filepath\"]).name\n",
    "    \n",
    "    state = None\n",
    "    chunk_phrases = []\n",
    "    for audio_chunk in read_stream_audio(path_to_file=audio_path, chunk_size=pipeline.CHUNK_SIZE):\n",
    "        new_phrases, state = pipeline.forward(audio_chunk, state)\n",
    "        if new_phrases:\n",
    "            chunk_phrases += new_phrases\n",
    "    # Finalize the pipeline and get the remaining phrases\n",
    "    new_phrases, _ = pipeline.finalize(state)\n",
    "    output = chunk_phrases + new_phrases\n",
    "    pred_text = \" \".join([phrase.text for phrase in output])\n",
    "    hypos.append(clean_text(pred_text))\n",
    "    \n",
    "    gts.append(clean_text(_meta[\"text\"]))\n",
    "\n",
    "all_hypos += hypos\n",
    "all_gts += gts\n",
    "word_error_rate(hypos, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b752cf6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15848406546080965"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_error_rate(all_hypos, all_gts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
