{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc464339",
   "metadata": {},
   "source": [
    "# Проверяем можно ли пересобрать модельку в onnx с 400мс буфером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb80855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle/T-one/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tone import StreamingCTCPipeline, read_stream_example_audio\n",
    "\n",
    "\n",
    "pipeline = StreamingCTCPipeline.from_hugging_face()\n",
    "\n",
    "# state = None  # Current state of the ASR pipeline (None - initial)\n",
    "# for audio_chunk in read_stream_example_audio():  # Use any source of audio chunks\n",
    "#     new_phrases, state = pipeline.forward(audio_chunk, state)\n",
    "#     # print(new_phrases)\n",
    "\n",
    "# # Finalize the pipeline and get the remaining phrases\n",
    "# new_phrases, _ = pipeline.finalize(state)\n",
    "# print(new_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eb4d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Хорошо, давай попробуем теперь скачать модельку локально и по-трейсить\n",
    "from pathlib import Path\n",
    "resources_path = Path(\"./model\")\n",
    "resources_path.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "# StreamingCTCPipeline.download_from_hugging_face(dir_path=\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca5865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -m tone.scripts.export --chunk-duration-ms 300 --output_path ./model_300ms.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4ac3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tone.scripts.export as tone_onnx_export\n",
    "\n",
    "path_to_pretrained = \"t-tech/T-one\"\n",
    "chunk_duration_ms = 300\n",
    "\n",
    "# model = tone_onnx_export.ModelToExport(path_to_pretrained, chunk_duration_ms)\n",
    "# model_bytes = tone_onnx_export._export_onnx(model)\n",
    "\n",
    "# output_path = Path(\"./model_300ms.onnx\")\n",
    "# output_path.write_bytes(model_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b39fc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal.shape=torch.Size([5, 2400, 1])\n",
      "Before encoder sub-sampling:\n",
      "inputs.shape=torch.Size([5, 64, 30])\n",
      "After encoder sub-sampling:\n",
      "audio_signal.shape=torch.Size([5, 10, 384]);length=None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 20, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 20])\n",
      "  state.att_mask.shape = torch.Size([5, 5, 20])\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 40, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 40])\n",
      "  state.att_mask.shape = torch.Size([5, 10, 40])\n",
      "signal.shape=torch.Size([5, 2400, 1])\n",
      "Before encoder sub-sampling:\n",
      "inputs.shape=torch.Size([5, 64, 30])\n",
      "After encoder sub-sampling:\n",
      "audio_signal.shape=torch.Size([5, 10, 384]);length=None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle/T-one/tone/nn/modules/conformer.py:213: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_audio_length=int(audio_signal.size(1)),\n",
      "/home/mle/T-one/tone/nn/modules/submodules.py:129: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 10, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 10])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 5, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 5])\n",
      "  state.att_mask is None\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 20, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 5, 20])\n",
      "  state.att_mask.shape = torch.Size([5, 5, 20])\n",
      "Attention forward:\n",
      "  v.shape = torch.Size([5, 8, 40, 48])\n",
      "  state.att_scores.shape = torch.Size([5, 8, 10, 40])\n",
      "  state.att_mask.shape = torch.Size([5, 10, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle/T-one/.venv/lib/python3.12/site-packages/torch/onnx/_internal/jit_utils.py:309: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/mle/T-one/.venv/lib/python3.12/site-packages/torch/onnx/utils.py:691: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/mle/T-one/.venv/lib/python3.12/site-packages/torch/onnx/utils.py:1161: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "144196083"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_duration_ms = 300\n",
    "\n",
    "model = tone_onnx_export.ModelToExport(path_to_pretrained, chunk_duration_ms)\n",
    "model_bytes = tone_onnx_export._export_onnx(model)\n",
    "\n",
    "output_path = Path(f\"./model_{chunk_duration_ms}ms.onnx\")\n",
    "output_path.write_bytes(model_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b29b00",
   "metadata": {},
   "source": [
    "# Проверяем метрики в стриминге\n",
    "\n",
    "как там wer считать ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b0ef5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mle/T-one/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[NeMo W 2025-12-17 12:51:58 nemo_logging:405] Megatron num_microbatches_calculator not found, using Apex version.\n",
      "OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
      "No exporters were provided. This means that no telemetry data will be collected.\n",
      "[NeMo W 2025-12-17 12:51:59 nemo_logging:405] /home/mle/T-one/.venv/lib/python3.12/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "\u001b[0;93m2025-12-17 12:52:00.357693747 [W:onnxruntime:, transformer_memcpy.cc:74 ApplyImpl] 38 Memcpy nodes are added to the graph main_graph for CUDAExecutionProvider. It might have negative impact on performance (including unable to run CUDA graph). Set session_options.log_severity_level=1 to see the detail logs before this message.\u001b[m\n",
      "\u001b[0;93m2025-12-17 12:52:00.365587087 [W:onnxruntime:, session_state.cc:1166 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-12-17 12:52:00.365593828 [W:onnxruntime:, session_state.cc:1168 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from tone import read_stream_audio, StreamingCTCPipeline, StreamingCTCModel\n",
    "from nemo.collections.asr.metrics.wer import word_error_rate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# StreamingCTCPipeline.download_from_hugging_face(\"./model\")\n",
    "# pipeline = StreamingCTCPipeline.from_local(\"./model\", providers=['CUDAExecutionProvider'])\n",
    "\n",
    "StreamingCTCModel.AUDIO_CHUNK_SAMPLES = int(0.4 * StreamingCTCModel.SAMPLE_RATE)\n",
    "StreamingCTCPipeline.CHUNK_SIZE = StreamingCTCModel.AUDIO_CHUNK_SAMPLES\n",
    "pipeline = StreamingCTCPipeline.from_local(\"./model_400ms_fixed\", providers=['CUDAExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17943a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cef5eb1c",
   "metadata": {},
   "source": [
    "# Теперь считаем нормальные тесты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ae2f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hypos = []\n",
    "all_gts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ebc637",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 264/264 [00:33<00:00,  7.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15128504672897197"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUB = str.maketrans('ё', 'е')\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Lowercase a string and substitute.\"\"\"\n",
    "    return s.lower().translate(SUB)\n",
    "base_data_path = Path(\"/home/mle/aiphoria-asr-training/data/rus_finetune\")\n",
    "\n",
    "dataset_name = \"test_collection\"\n",
    "# load collections manifest\n",
    "with open(base_data_path / dataset_name / \"tarred_audio_manifest.json\") as fin:\n",
    "    jdata = []\n",
    "    for line in fin:\n",
    "        jline = json.loads(line.strip())\n",
    "        jdata.append(jline)\n",
    "len(jdata)\n",
    "\n",
    "hypos = []\n",
    "gts = []\n",
    "for _meta in tqdm(jdata):\n",
    "    audio_path = base_data_path / dataset_name / Path(_meta[\"audio_filepath\"]).name\n",
    "    \n",
    "    state = None\n",
    "    chunk_phrases = []\n",
    "    for audio_chunk in read_stream_audio(path_to_file=audio_path, chunk_size=pipeline.CHUNK_SIZE):\n",
    "        new_phrases, state = pipeline.forward(audio_chunk, state)\n",
    "        if new_phrases:\n",
    "            chunk_phrases += new_phrases\n",
    "    # Finalize the pipeline and get the remaining phrases\n",
    "    new_phrases, _ = pipeline.finalize(state)\n",
    "    output = chunk_phrases + new_phrases\n",
    "    pred_text = \" \".join([phrase.text for phrase in output])\n",
    "    hypos.append(clean_text(pred_text))\n",
    "    \n",
    "    gts.append(clean_text(_meta[\"text\"]))\n",
    "\n",
    "all_hypos += hypos\n",
    "all_gts += gts\n",
    "word_error_rate(hypos, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642581b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311/311 [00:55<00:00,  5.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12844488188976377"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"test_rec_support\"\n",
    "# load collections manifest\n",
    "with open(base_data_path / dataset_name / \"tarred_audio_manifest.json\") as fin:\n",
    "    jdata = []\n",
    "    for line in fin:\n",
    "        jline = json.loads(line.strip())\n",
    "        jdata.append(jline)\n",
    "len(jdata)\n",
    "\n",
    "hypos = []\n",
    "gts = []\n",
    "for _meta in tqdm(jdata):\n",
    "    audio_path = base_data_path / dataset_name / Path(_meta[\"audio_filepath\"]).name\n",
    "    \n",
    "    state = None\n",
    "    chunk_phrases = []\n",
    "    for audio_chunk in read_stream_audio(path_to_file=audio_path, chunk_size=pipeline.CHUNK_SIZE):\n",
    "        new_phrases, state = pipeline.forward(audio_chunk, state)\n",
    "        if new_phrases:\n",
    "            chunk_phrases += new_phrases\n",
    "    # Finalize the pipeline and get the remaining phrases\n",
    "    new_phrases, _ = pipeline.finalize(state)\n",
    "    output = chunk_phrases + new_phrases\n",
    "    pred_text = \" \".join([phrase.text for phrase in output])\n",
    "    hypos.append(clean_text(pred_text))\n",
    "    \n",
    "    gts.append(clean_text(_meta[\"text\"]))\n",
    "\n",
    "all_hypos += hypos\n",
    "all_gts += gts\n",
    "word_error_rate(hypos, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e419100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 314/314 [00:37<00:00,  8.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19408054342552158"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"test_support\"\n",
    "# load collections manifest\n",
    "with open(base_data_path / dataset_name / \"tarred_audio_manifest.json\") as fin:\n",
    "    jdata = []\n",
    "    for line in fin:\n",
    "        jline = json.loads(line.strip())\n",
    "        jdata.append(jline)\n",
    "len(jdata)\n",
    "\n",
    "hypos = []\n",
    "gts = []\n",
    "for _meta in tqdm(jdata):\n",
    "    audio_path = base_data_path / dataset_name / Path(_meta[\"audio_filepath\"]).name\n",
    "    \n",
    "    state = None\n",
    "    chunk_phrases = []\n",
    "    for audio_chunk in read_stream_audio(path_to_file=audio_path, chunk_size=pipeline.CHUNK_SIZE):\n",
    "        new_phrases, state = pipeline.forward(audio_chunk, state)\n",
    "        if new_phrases:\n",
    "            chunk_phrases += new_phrases\n",
    "    # Finalize the pipeline and get the remaining phrases\n",
    "    new_phrases, _ = pipeline.finalize(state)\n",
    "    output = chunk_phrases + new_phrases\n",
    "    pred_text = \" \".join([phrase.text for phrase in output])\n",
    "    hypos.append(clean_text(pred_text))\n",
    "    \n",
    "    gts.append(clean_text(_meta[\"text\"]))\n",
    "\n",
    "all_hypos += hypos\n",
    "all_gts += gts\n",
    "word_error_rate(hypos, gts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b752cf6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15848406546080965"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_error_rate(all_hypos, all_gts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
